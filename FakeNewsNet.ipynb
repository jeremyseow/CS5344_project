{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637d17b1",
   "metadata": {},
   "source": [
    "## FakeNewsNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af22c9b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports\n",
    "import os.path\n",
    "import urllib.request\n",
    "import shutil\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "# external library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# project imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ea7d7",
   "metadata": {},
   "source": [
    "### PySpark contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2467a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FakeNewsNet').getOrCreate()\n",
    "sparkcontext = spark.sparkContext\n",
    "sqlcontext   = SQLContext(sparkcontext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56b3e7",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# download or reference the dataset\n",
    "# ----------------------------------------\n",
    "def get_or_download_dataset():\n",
    "    path = \"./dataset/tweets\"\n",
    "    if not os.path.exists(\"./dataset\"):\n",
    "        print(\"downloading tweets...\")\n",
    "        os.mkdir(\"./dataset\")\n",
    "        # lionel: no public access to download the dataset directly. manually download and place in the stated directory\n",
    "        url = \"https://nusu-my.sharepoint.com/:u:/g/personal/e0809358_u_nus_edu/ETPkp1-0GbBHgB__wyeCS_QBE7_SFluzSCtocU0mUr3Jng\"\n",
    "        with urllib.request.urlopen(url) as response, open(path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "            with zipfile.ZipFile(path + \"/results.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extractall(path)\n",
    "    return path\n",
    "# ----------------------------------------\n",
    "# convert to pandas dataframe\n",
    "# ----------------------------------------\n",
    "def to_dataframe_pandas(path, head_len = 10000):\n",
    "    directories = os.listdir(path)\n",
    "    json_all = []\n",
    "    df_all = pd.DataFrame\n",
    "    print(\"reading first \" + str(head_len) + \" json files in \" + str(len(directories)) + \" directories in \" + path + \"...\")\n",
    "    for index,directory in enumerate(directories):\n",
    "        path_prefix = path + \"/\" + directory + \"/tweets\"\n",
    "        files = os.listdir(path_prefix)\n",
    "        # print(\"reading \" + str(len(files)) + \" json files from directory \" + directory + \" (\" + str(index) + \" of \" + str(len(directories)) + \")\")\n",
    "        json_lines = []\n",
    "        for file in files:\n",
    "            path_full = path_prefix + \"/\" + file\n",
    "            with open(path_full, 'r') as json_file:\n",
    "                json_lines.append(json.loads(json_file.read()))\n",
    "        df_current = pd.json_normalize(json_lines)\n",
    "        assert len(files) == len(json_lines)\n",
    "        assert len(files) == df_current.shape[0]\n",
    "        # lionel: fixed schema with columns we want must be known beforehand as columns are mismatched among json files\n",
    "        # df_all = pd.concat([df_all, df_current], axis=0, join='outer', sort=False)\n",
    "        json_all.extend(json_lines)\n",
    "        if head_len is not None and len(json_all) >= head_len:\n",
    "            break\n",
    "    df_all = pd.json_normalize(json_all[:head_len])\n",
    "    return df_all\n",
    "# ----------------------------------------\n",
    "# convert to spark dataframe\n",
    "# ----------------------------------------\n",
    "def to_dataframe_spark(path, head_len = 10000):\n",
    "    # lionel: out-of-memory\n",
    "    # df_all = sqlcontext.read.json(path + \"/*/tweets/*.json\")\n",
    "    # return df_all\n",
    "    directories = os.listdir(path)\n",
    "    json_files = []\n",
    "    df_all = spark.createDataFrame([], StructType([]))\n",
    "    print(\"reading first \" + str(head_len) + \" json files in \" + str(len(directories)) + \" directories in \" + path + \"...\")\n",
    "    for index,directory in enumerate(directories):\n",
    "        path_prefix = path + \"/\" + directory + \"/tweets\"\n",
    "        files = os.listdir(path_prefix)\n",
    "        # print(\"reading \" + str(len(files)) + \" json files from directory \" + directory + \" (\" + str(index) + \" of \" + str(len(directories)) + \")\")\n",
    "        for file in files:\n",
    "            json_files.append(path_prefix + \"/\" + file)\n",
    "        # lionel: fixed schema with columns we want must be known beforehand as columns are mismatched among json files\n",
    "        # df_current = sqlcontext.read.json(path_prefix + \"/*.json\")\n",
    "        # assert len(files) == df_current.count()\n",
    "        # df_all = df_all.unionByName(df_current, allowMissingColumns=True)\n",
    "        if head_len is not None and len(json_files) >= head_len:\n",
    "            break\n",
    "    df_all = sqlcontext.read.json(json_files)\n",
    "    assert len(json_files) == df_all.count()\n",
    "    if head_len is not None:\n",
    "        df_all = df_all.limit(head_len)\n",
    "    return df_all\n",
    "# ----------------------------------------\n",
    "# print helper function\n",
    "# ----------------------------------------\n",
    "def print_df(dfs):\n",
    "    for df in dfs:\n",
    "        print(\"==========\")\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            print(df.shape)\n",
    "            # print(df.info())\n",
    "            # print(df.columns)\n",
    "            # print(df.describe())\n",
    "        if isinstance(df, DataFrame):\n",
    "            print(\"(\" + str(df.count()) + \",\" + str(len(df.columns)) + \")\")\n",
    "            # print(df.columns)\n",
    "            # print(df.summary().show())\n",
    "    print(\"==========\")\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "!pwd\n",
    "!ls -l\n",
    "path_dataset = get_or_download_dataset()\n",
    "df_fake_1 = to_dataframe_spark(path_dataset + \"/gossipcop\" + \"/fake\")\n",
    "df_real_1 = to_dataframe_spark(path_dataset + \"/gossipcop\" + \"/real\")\n",
    "df_fake_2 = to_dataframe_spark(path_dataset + \"/politifact\" + \"/fake\")\n",
    "df_real_2 = to_dataframe_spark(path_dataset + \"/politifact\" + \"/real\")\n",
    "df_fake_1 = df_fake_1.toPandas()\n",
    "df_real_1 = df_real_1.toPandas()\n",
    "df_fake_2 = df_fake_2.toPandas()\n",
    "df_real_2 = df_real_2.toPandas()\n",
    "print_df([df_fake_1, df_real_1, df_fake_2, df_real_2])\n",
    "# lionel: weirdly, pandas' and spark's json APIs yield different number of columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e92906",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2491bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
