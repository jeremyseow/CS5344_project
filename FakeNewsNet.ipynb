{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637d17b1",
   "metadata": {},
   "source": [
    "## FakeNewsNet (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af22c9b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# external library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from efficient_apriori import apriori\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bd847",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# download and unzip dataset if missing\n",
    "# ----------------------------------------\n",
    "def get_or_download_dataset():\n",
    "    path_dataset       = \"./dataset\"\n",
    "    path_group_by_user = \"./dataset/group_by_user\"\n",
    "    path_unique_users  = \"./dataset/unique_users\"\n",
    "    if not os.path.exists(path_dataset):\n",
    "        print(\"creating directories here...\")\n",
    "        !pwd\n",
    "        os.mkdir(path_dataset)\n",
    "        os.mkdir(path_group_by_user)\n",
    "        os.mkdir(path_unique_users)\n",
    "        print(\"downloading dataset...\")\n",
    "        # https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99\n",
    "        file_link_1 = 'https://drive.google.com/file/d/1UBFC0m5F4sln-YSP3zkq-5__ZJVzycme/view?usp=sharing'\n",
    "        file_link_2 = 'https://drive.google.com/file/d/1gXmSAoH-gT7fAcq0g16tnb-fx1hTDv5M/view?usp=sharing'\n",
    "        file_link_3 = 'https://drive.google.com/file/d/1KpTogf6HIgicXDjrEANjAO4Bg-5zQ2Iq/view?usp=sharing'\n",
    "        file_link_4 = 'https://drive.google.com/file/d/1C09l3Mq7SrzJ1hYFK6Y2kbRbZsmko3Ew/view?usp=sharing'\n",
    "        file_id_1   = '1UBFC0m5F4sln-YSP3zkq-5__ZJVzycme'\n",
    "        file_id_2   = '1gXmSAoH-gT7fAcq0g16tnb-fx1hTDv5M'\n",
    "        file_id_3   = '1KpTogf6HIgicXDjrEANjAO4Bg-5zQ2Iq'\n",
    "        file_id_4   = '1C09l3Mq7SrzJ1hYFK6Y2kbRbZsmko3Ew'\n",
    "        file_name_1 = 'gossipcop.zip'\n",
    "        file_name_2 = 'politifact.zip'\n",
    "        file_name_3 = 'gc_all.csv'\n",
    "        file_name_4 = 'pf_all.csv'\n",
    "        !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UBFC0m5F4sln-YSP3zkq-5__ZJVzycme' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1UBFC0m5F4sln-YSP3zkq-5__ZJVzycme\" -O gossipcop.zip  && rm -rf /tmp/cookies.txt\n",
    "        !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1gXmSAoH-gT7fAcq0g16tnb-fx1hTDv5M' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1gXmSAoH-gT7fAcq0g16tnb-fx1hTDv5M\" -O politifact.zip && rm -rf /tmp/cookies.txt\n",
    "        !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1KpTogf6HIgicXDjrEANjAO4Bg-5zQ2Iq' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1KpTogf6HIgicXDjrEANjAO4Bg-5zQ2Iq\" -O gc_all.csv     && rm -rf /tmp/cookies.txt\n",
    "        !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1C09l3Mq7SrzJ1hYFK6Y2kbRbZsmko3Ew' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1C09l3Mq7SrzJ1hYFK6Y2kbRbZsmko3Ew\" -O pf_all.csv     && rm -rf /tmp/cookies.txt\n",
    "        !ls -l\n",
    "        print(\"extracting dataset...\")\n",
    "        with zipfile.ZipFile(file_name_1, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path_group_by_user)\n",
    "            !rm 'gossipcop.zip'\n",
    "        with zipfile.ZipFile(file_name_2, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path_group_by_user)\n",
    "            !rm 'politifact.zip'\n",
    "        !mv gc_all.csv ./dataset/unique_users\n",
    "        !mv pf_all.csv ./dataset/unique_users\n",
    "        print(\"dataset successfully downloaded and extracted!\")\n",
    "    else:\n",
    "        print(\"dataset path exists! skipping...\")\n",
    "    return path_dataset\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "path_dataset = get_or_download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1ab8e",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30320184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# convert to spark rdd from json\n",
    "# ----------------------------------------\n",
    "def to_rdd_spark(sqlcontext, path, head_len = None):\n",
    "    if head_len is None:\n",
    "        print(\"reading \" + path + \"...\")\n",
    "        df_all = sqlcontext.read.json(path + \"/union/*.json\")\n",
    "        return df_all\n",
    "    directories = os.listdir(path)\n",
    "    json_files = []\n",
    "    df_all = spark.createDataFrame([], StructType([]))\n",
    "    print(\"reading first \" + str(head_len) + \" json files in \" + str(len(directories)) + \" directories in \" + path + \"...\")\n",
    "    for index,directory in enumerate(directories):\n",
    "        path_prefix = path + \"/\" + directory + \"/tweets\"\n",
    "        files = os.listdir(path_prefix)\n",
    "        for file in files:\n",
    "            json_files.append(path_prefix + \"/\" + file)\n",
    "        if head_len is not None and len(json_files) >= head_len:\n",
    "            break\n",
    "    if head_len is not None:\n",
    "        df_all = sqlcontext.read.json(json_files[:head_len])\n",
    "        assert len(json_files[:head_len]) == df_all.count()\n",
    "    else:\n",
    "        df_all = sqlcontext.read.json(json_files)\n",
    "    return df_all\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "sparksession = SparkSession.builder.appName('FakeNewsNet').getOrCreate()\n",
    "sparkcontext = sparksession.sparkContext\n",
    "sqlcontext   = SQLContext(sparkcontext)\n",
    "\n",
    "print(\"reading RDDs...\")\n",
    "dd_fake_1 = to_rdd_spark(sqlcontext, path_dataset + \"/group_by_user\" + \"/gossipcop\"  + \"/fake\")\n",
    "dd_real_1 = to_rdd_spark(sqlcontext, path_dataset + \"/group_by_user\" + \"/gossipcop\"  + \"/real\")\n",
    "dd_fake_2 = to_rdd_spark(sqlcontext, path_dataset + \"/group_by_user\" + \"/politifact\" + \"/fake\")\n",
    "dd_real_2 = to_rdd_spark(sqlcontext, path_dataset + \"/group_by_user\" + \"/politifact\" + \"/real\")\n",
    "\n",
    "print(\"reading Dataframes...\")\n",
    "df_fake_1 = dd_fake_1.toPandas()\n",
    "df_real_1 = dd_real_1.toPandas()\n",
    "df_fake_2 = dd_fake_2.toPandas()\n",
    "df_real_2 = dd_real_2.toPandas()\n",
    "\n",
    "print(\"reading CSVs...\")\n",
    "df_uniq_1 = pd.read_csv(\"./dataset/unique_users/gc_all.csv\")\n",
    "df_uniq_2 = pd.read_csv(\"./dataset/unique_users/pf_all.csv\")\n",
    "\n",
    "print(\"reading done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ea7d7",
   "metadata": {},
   "source": [
    "### Print Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2467a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# print raw data\n",
    "# ----------------------------------------\n",
    "def print_dataset(dataset):\n",
    "    for data in dataset:\n",
    "        if isinstance(data, RDD):\n",
    "            myRDD.take(20).foreach(println)\n",
    "        if isinstance(data, DataFrame):\n",
    "            print(\"(\" + str(data.count()) + \",\" + str(len(data.columns)) + \")\")\n",
    "            print(data.columns)\n",
    "            print(data.summary().show())\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            print(data.shape)\n",
    "            print(data.info())\n",
    "            print(data.columns)\n",
    "            print(data.describe())\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "print(\"========== RDDs ==========\")\n",
    "print_dataset([dd_fake_1, dd_real_1, dd_fake_2, dd_real_2])\n",
    "print(\"========== DFs ==========\")\n",
    "print_dataset([df_fake_1, df_real_1, df_fake_2, df_real_2])\n",
    "print(\"========== CSVs ==========\")\n",
    "print_dataset([df_uniq_1, df_uniq_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56b3e7",
   "metadata": {},
   "source": [
    "### Pie Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# visualize pie chart\n",
    "# ----------------------------------------\n",
    "def visualize_pie_chart(df_fake, df_real, column_name, labels):\n",
    "    df_sum_fake = df_fake[column_name].sum()\n",
    "    df_sum_real = df_real[column_name].sum()\n",
    "    list_sums   = np.array([df_sum_fake, df_sum_real])\n",
    "    plt.pie(list_sums, labels = labels, autopct='%1.1f%%')\n",
    "    plt.show()\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "print(\"========== num_of_tweets ==========\")\n",
    "visualize_pie_chart(df_fake_1, df_real_1, \"num_of_tweets\", [\"Gossipcop Fake\", \"Gossipcop Real\"])\n",
    "visualize_pie_chart(df_fake_2, df_real_2, \"num_of_tweets\", [\"Politifact Fake\", \"Politifact Real\"])\n",
    "\n",
    "print(\"========== total_favorite_count ==========\")\n",
    "visualize_pie_chart(df_fake_1, df_real_1, \"total_favorite_count\", [\"Gossipcop Fake\", \"Gossipcop Real\"])\n",
    "visualize_pie_chart(df_fake_2, df_real_2, \"total_favorite_count\", [\"Politifact Fake\", \"Politifact Real\"])\n",
    "\n",
    "print(\"========== total_retweet_count ==========\")\n",
    "visualize_pie_chart(df_fake_1, df_real_1, \"total_retweet_count\", [\"Gossipcop Fake\", \"Gossipcop Real\"])\n",
    "visualize_pie_chart(df_fake_2, df_real_2, \"total_retweet_count\", [\"Politifact Fake\", \"Politifact Real\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6822e7",
   "metadata": {},
   "source": [
    "### Histogram Bins of Follower Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d342435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# visualize histogram bin\n",
    "# ----------------------------------------\n",
    "def visualize_histogram_bin(df_uniq, column_name, title):\n",
    "    bin_values = [100, 1000, 10000, 100000, 1000000, 1000000]\n",
    "    bin_names  = [\"<100\", \"100-1k\", \"1k-10k\", \"10k-100k\", \"100k-1m\", \">10m\"]\n",
    "    x_values   = np.arange(len(bin_names))\n",
    "    y_values   = []\n",
    "    \n",
    "    df_notna = df_uniq[df_uniq[column_name].notna()]\n",
    "    df_first = df_notna[df_notna[column_name] < bin_values[0]]\n",
    "    y_values.append(len(df_first))\n",
    "    for index in range(1, len(bin_values)):\n",
    "        df_current = df_notna[(df_notna[column_name] >= bin_values[index - 1]) & (df_notna[column_name] < bin_values[index])]\n",
    "        y_values.append(len(df_current))\n",
    "\n",
    "    plt.bar(x_values, y_values, 0.2, label=column_name)\n",
    "    plt.xticks(x_values, bin_names)\n",
    "    plt.xlabel(column_name)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "visualize_histogram_bin(df_uniq_1, \"followers_count\", \"Histogram of Followers Count (Gossipcop)\")\n",
    "visualize_histogram_bin(df_uniq_2, \"followers_count\", \"Histogram of Followers Count (Politifact)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f8ed9",
   "metadata": {},
   "source": [
    "### Favorites vs Retweets Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Favorites vs Retweets\n",
    "# ----------------------------------------\n",
    "def favorite_vs_retweet(df, title):\n",
    "    X_val = df['favourites_count']\n",
    "    Y_val = df['total_retweet_count']\n",
    "    plt.figure()\n",
    "    plt.tick_params(labelsize=14)\n",
    "    plt.scatter(X_val, Y_val)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('favourites_count', fontsize=12)\n",
    "    plt.ylabel('total_retweet_count', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "favorite_vs_retweet(df_fake_1, \"gossipcop fake\")\n",
    "favorite_vs_retweet(df_real_1, \"gossipcop real\")\n",
    "favorite_vs_retweet(df_fake_2, \"politifact fake\")\n",
    "favorite_vs_retweet(df_real_2, \"politifact real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924ae03",
   "metadata": {},
   "source": [
    "### Retweets Fake vs Real Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Retweets Fake vs Real Chart\n",
    "# ----------------------------------------\n",
    "def retweets_fake_vs_real(rdd_fake, rdd_real, ceiling = None):\n",
    "    df_retweet_fake = rdd_fake.groupBy(\"user_id\").sum(\"total_retweet_count\").toPandas()\n",
    "    df_retweet_real = rdd_real.groupBy(\"user_id\").sum(\"total_retweet_count\").toPandas()\n",
    "    if(ceiling is not None):\n",
    "        df_retweet_fake = df_retweet_fake.drop(df_retweet_fake[df_retweet_fake[\"sum(total_retweet_count)\"] > ceiling].index)\n",
    "        df_retweet_real = df_retweet_real.drop(df_retweet_real[df_retweet_real[\"sum(total_retweet_count)\"] > ceiling].index)\n",
    "    df_fake_vs_real = pd.merge(df_retweet_fake, df_retweet_real, on='user_id', how='outer')\n",
    "    df_fake_vs_real['total_retweets'] = (df_fake_vs_real['sum(total_retweet_count)_x'].fillna(0) + df_fake_vs_real['sum(total_retweet_count)_y'].fillna(0))\n",
    "    return df_fake_vs_real\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "# must call in separate cells for chart to render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db369ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========== Gossipcop ==========\")\n",
    "df_fake_vs_real_gc = retweets_fake_vs_real(dd_fake_1, dd_real_1)\n",
    "p = ggplot(aes(x='sum(total_retweet_count)_x', y='sum(total_retweet_count)_y'), df_fake_vs_real_gc)\n",
    "p + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========== Gossipcop Zoom In ==========\")\n",
    "df_fake_vs_real_gc_zoom = retweets_fake_vs_real(dd_fake_1, dd_real_1, 50)\n",
    "p = ggplot(aes(x='sum(total_retweet_count)_x', y='sum(total_retweet_count)_y'), df_fake_vs_real_gc_zoom)\n",
    "p + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========== Politifact ==========\")\n",
    "df_fake_vs_real_pf = retweets_fake_vs_real(dd_fake_2, dd_real_2)\n",
    "p = ggplot(aes(x='sum(total_retweet_count)_x', y='sum(total_retweet_count)_y'), df_fake_vs_real_pf)\n",
    "p + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========== Politifact Zoom In ==========\")\n",
    "df_fake_vs_real_pf_zoom = retweets_fake_vs_real(dd_fake_2, dd_real_2, 50)\n",
    "p = ggplot(aes(x='sum(total_retweet_count)_x', y='sum(total_retweet_count)_y'), df_fake_vs_real_pf_zoom)\n",
    "p + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ab4a8",
   "metadata": {},
   "source": [
    "### Retweets Fake vs Real Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c88bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# visualize top retweeters distribution\n",
    "# ----------------------------------------\n",
    "def visualize_retweeters_topk(df_fake_real, dataset_name, topk):\n",
    "    df_topk   = df_fake_real.sort_values(\"total_retweets\", ascending = False)\n",
    "    df_topk   = df_topk.head(topk)\n",
    "    bin_names = np.arange(1, topk + 1)\n",
    "    x_values  = np.arange(len(bin_names))\n",
    "    topk_fake = df_topk[\"sum(total_retweet_count)_x\"]\n",
    "    topk_real = df_topk[\"sum(total_retweet_count)_y\"]\n",
    "\n",
    "    plt.bar(x_values - 0.2, topk_fake, 0.4, label = 'Fake')\n",
    "    plt.bar(x_values + 0.2, topk_real, 0.4, label = 'Real')\n",
    "    plt.xticks(x_values, bin_names)\n",
    "    plt.ylabel(\"Number of retweets\")\n",
    "    plt.title(dataset_name + \" Top \" + str(topk) + \" users by number of retweets\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "visualize_retweeters_topk(df_fake_vs_real_gc, \"Gossipcop\", 20)\n",
    "visualize_retweeters_topk(df_fake_vs_real_pf, \"Politifact\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec6541",
   "metadata": {},
   "source": [
    "### Tweet Word Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ee7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# count frequency of words in tweets\n",
    "# ----------------------------------------\n",
    "def tweet_word_counter(df, sample_percent):\n",
    "    col_value = df['list_of_tweets']\n",
    "    row_count = len(col_value)\n",
    "    word_freq = Counter([])\n",
    "    tweet_tokenlist = []\n",
    "    complete_percent = 0\n",
    "    for index, tweets in enumerate(col_value):\n",
    "        for curr_tweet in tweets:\n",
    "            curr_tweet_text = re.search(r\"^.*?,\\\"text\\\":\\\"(.*)\\\",\\\"truncated\\\".*$\", curr_tweet).group(1)\n",
    "            words = curr_tweet_text.split()\n",
    "            words = [word.lower() for word in words]\n",
    "            word_freq = word_freq + Counter(words)\n",
    "            if False:\n",
    "                tweet_tokenlist.append(tuple(set(word_freq)))\n",
    "        if index % 100 == 0:\n",
    "            complete_percent += round((index / row_count) * 100, 2)\n",
    "            print(str(complete_percent) + \"% of dataset\")\n",
    "            if sample_percent is not None and complete_percent > sample_percent:\n",
    "                break\n",
    "    return word_freq, tweet_tokenlist\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "print(\"processing Gossipcop Fake...\")\n",
    "counter_fake_1, tokenlist_fake_1 = tweet_word_counter(df_fake_1, 5)\n",
    "print(\"processing Gossipcop Real...\")\n",
    "counter_real_1, tokenlist_real_1 = tweet_word_counter(df_real_1, 5)\n",
    "print(\"processing Politifact Fake...\")\n",
    "counter_fake_2, tokenlist_fake_2 = tweet_word_counter(df_fake_2, 5)\n",
    "print(\"processing Politifact Real...\")\n",
    "counter_real_2, tokenlist_real_2 = tweet_word_counter(df_real_2, 5)\n",
    "print(\"processing done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686acf2e",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba70cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# additional stopwords to remove\n",
    "# ----------------------------------------\n",
    "additional_stopwords = {\n",
    "    'to', 'a', 'the', 'and', 'on', '-', 'is', 'with',\n",
    "    'i', 'of', 'you', '&amp;', 'for', 'out', 'air',\n",
    "    'win', 'in', 'her', 'tv', 'at', 'after', 'best',\n",
    "    'your', 'first', 'new', 'about', 'found', 'as',\n",
    "    'how', 'via', 'my', 'by', 'if', 'one', 'four',\n",
    "    'that', 'have', 'was', 'this', 'are', 'be', 'it',\n",
    "    'has', 'from', 'all', 'but', 'just', 'not', 'u.s.',\n",
    "    'only', 'two', 'more', 'will', 'an', 'me', 'had',\n",
    "    'like', 'we', 'so', 'been', 'our', 'or', 'three',\n",
    "    \"i'm\", 'years', 'see', 'top', '#eonline', '2018',\n",
    "    'says', 'us', 'https', 'news', 't', 'co', 's',\n",
    "    'https://t.co/g79yrlmstd', 'https://t.co/pas8l48opb'\n",
    "    }\n",
    "def filter_stopwords(counter, stopwords):\n",
    "\n",
    "    for stopword in stopwords:\n",
    "        del counter[stopword]\n",
    "        del counter[stopword]\n",
    "        del counter[stopword]\n",
    "        del counter[stopword]\n",
    "def show_top_words(counter, topk = 20):\n",
    "    print(\"number of unqiue words: \" + str(len(counter)))\n",
    "    topk_words = counter.most_common(topk)\n",
    "    print(\"most common words:\")\n",
    "    print(*topk_words, sep = \"\\n\")\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "stopwords  = set(STOPWORDS)\n",
    "stopwords |= additional_stopwords\n",
    "    \n",
    "filter_stopwords(counter_fake_1, stopwords)\n",
    "filter_stopwords(counter_real_1, stopwords)\n",
    "filter_stopwords(counter_fake_2, stopwords)\n",
    "filter_stopwords(counter_real_2, stopwords)\n",
    "\n",
    "print(\"========== Gossipcop Fake ==========\")\n",
    "show_top_words(counter_fake_1)\n",
    "print(\"========== Gossipcop Real ==========\")\n",
    "show_top_words(counter_real_1)\n",
    "print(\"========== Politifact Fake ==========\")\n",
    "show_top_words(counter_fake_2)\n",
    "print(\"========== Politifact Real ==========\")\n",
    "show_top_words(counter_real_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb28abb8",
   "metadata": {},
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# visualize using word cloud\n",
    "# ----------------------------------------\n",
    "def visualize_word_cloud(counter, stopwords):\n",
    "    words = \" \".join(counter.elements())\n",
    "    wordcloud = WordCloud(\n",
    "        width = 800, height = 800, background_color ='white',\n",
    "        collocations = False, stopwords = stopwords,\n",
    "        min_font_size = 10).generate(words)\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "print(\"========== Gossipcop Fake ==========\")\n",
    "visualize_word_cloud(counter_fake_1, stopwords)\n",
    "print(\"========== Gossipcop Real ==========\")\n",
    "visualize_word_cloud(counter_real_1, stopwords)\n",
    "print(\"========== Politifact Fake ==========\")\n",
    "visualize_word_cloud(counter_fake_2, stopwords)\n",
    "print(\"========== Politifact Real ==========\")\n",
    "visualize_word_cloud(counter_real_2, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b2d4b",
   "metadata": {},
   "source": [
    "### Association Rules with Top Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9506d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Skipped - takes too long\n",
    "# ----------------------------------------\n",
    "# itemsets, rules = apriori(tokenlist_fake_1, min_support=0.9, min_confidence=0.9)\n",
    "# itemsets, rules = apriori(tokenlist_real_1, min_support=0.9, min_confidence=0.9)\n",
    "# itemsets, rules = apriori(tokenlist_fake_2, min_support=0.9, min_confidence=0.9)\n",
    "# itemsets, rules = apriori(tokenlist_real_2, min_support=0.9, min_confidence=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
