{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637d17b1",
   "metadata": {},
   "source": [
    "## FakeNewsNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af22c9b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports\n",
    "import os.path\n",
    "import urllib.request\n",
    "import shutil\n",
    "import zipfile\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# external library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.rdd import RDD\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "# project imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ea7d7",
   "metadata": {},
   "source": [
    "### PySpark contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2467a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FakeNewsNet').getOrCreate()\n",
    "sparkcontext = spark.sparkContext\n",
    "sqlcontext   = SQLContext(sparkcontext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56b3e7",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# download or reference the dataset\n",
    "# ----------------------------------------\n",
    "def get_or_download_dataset():\n",
    "    path = \"./dataset/tweets\"\n",
    "    if not os.path.exists(\"./dataset\"):\n",
    "        print(\"downloading tweets...\")\n",
    "        os.mkdir(\"./dataset\")\n",
    "        # lionel: no public access to download the dataset directly. manually download and place in the stated directory\n",
    "        url = \"https://nusu-my.sharepoint.com/:u:/g/personal/e0809358_u_nus_edu/ETPkp1-0GbBHgB__wyeCS_QBE7_SFluzSCtocU0mUr3Jng\"\n",
    "        with urllib.request.urlopen(url) as response, open(path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "            with zipfile.ZipFile(path + \"/results.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extractall(path)\n",
    "    return path\n",
    "# ----------------------------------------\n",
    "# convert to pandas dataframe\n",
    "# ----------------------------------------\n",
    "def to_dataframe_pandas(path, head_len = None):\n",
    "    directories = os.listdir(path)\n",
    "    json_all = []\n",
    "    df_all = pd.DataFrame\n",
    "    print(\"reading first \" + str(head_len) + \" json files in \" + str(len(directories)) + \" directories in \" + path + \"...\")\n",
    "    for index,directory in enumerate(directories):\n",
    "        path_prefix = path + \"/\" + directory + \"/tweets\"\n",
    "        files = os.listdir(path_prefix)\n",
    "        # print(\"reading \" + str(len(files)) + \" json files from directory \" + directory + \" (\" + str(index) + \" of \" + str(len(directories)) + \")\")\n",
    "        json_lines = []\n",
    "        for file in files:\n",
    "            path_full = path_prefix + \"/\" + file\n",
    "            with open(path_full, 'r') as json_file:\n",
    "                json_lines.append(json.loads(json_file.read()))\n",
    "        df_current = pd.json_normalize(json_lines)\n",
    "        assert len(files) == len(json_lines)\n",
    "        assert len(files) == df_current.shape[0]\n",
    "        # lionel: fixed schema with columns we want must be known beforehand as columns are mismatched among json files\n",
    "        # df_all = pd.concat([df_all, df_current], axis=0, join='outer', sort=False)\n",
    "        json_all.extend(json_lines)\n",
    "        if head_len is not None and len(json_all) >= head_len:\n",
    "            break\n",
    "    if head_len is not None:\n",
    "        df_all = pd.json_normalize(json_all[:head_len])\n",
    "    else:\n",
    "        df_all = pd.json_normalize(json_all)\n",
    "    return df_all\n",
    "# ----------------------------------------\n",
    "# convert to spark dataframe\n",
    "# ----------------------------------------\n",
    "def to_dataframe_spark(path, head_len = None):\n",
    "    # lionel: out-of-memory\n",
    "    if head_len is None:\n",
    "        df_all = sqlcontext.read.json(path + \"/*/tweets/*.json\")\n",
    "        return df_all\n",
    "    directories = os.listdir(path)\n",
    "    json_files = []\n",
    "    df_all = spark.createDataFrame([], StructType([]))\n",
    "    print(\"reading first \" + str(head_len) + \" json files in \" + str(len(directories)) + \" directories in \" + path + \"...\")\n",
    "    for index,directory in enumerate(directories):\n",
    "        path_prefix = path + \"/\" + directory + \"/tweets\"\n",
    "        files = os.listdir(path_prefix)\n",
    "        # print(\"reading \" + str(len(files)) + \" json files from directory \" + directory + \" (\" + str(index) + \" of \" + str(len(directories)) + \")\")\n",
    "        for file in files:\n",
    "            json_files.append(path_prefix + \"/\" + file)\n",
    "        # lionel: fixed schema with columns we want must be known beforehand as columns are mismatched among json files\n",
    "        # df_current = sqlcontext.read.json(path_prefix + \"/*.json\")\n",
    "        # assert len(files) == df_current.count()\n",
    "        # df_all = df_all.unionByName(df_current, allowMissingColumns=True)\n",
    "        if head_len is not None and len(json_files) >= head_len:\n",
    "            break\n",
    "    if head_len is not None:\n",
    "        df_all = sqlcontext.read.json(json_files[:head_len])\n",
    "        assert len(json_files[:head_len]) == df_all.count()\n",
    "    else:\n",
    "        df_all = sqlcontext.read.json(json_files)\n",
    "    return df_all\n",
    "# ----------------------------------------\n",
    "# print helper function\n",
    "# ----------------------------------------\n",
    "def print_df(dfs):\n",
    "    for df in dfs:\n",
    "        print(\"==========\")\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            print(df.shape)\n",
    "            print(df.info())\n",
    "            print(df.columns)\n",
    "            print(df.describe())\n",
    "        if isinstance(df, DataFrame):\n",
    "            print(\"(\" + str(df.count()) + \",\" + str(len(df.columns)) + \")\")\n",
    "            print(df.columns)\n",
    "            print(df.summary().show())\n",
    "    print(\"==========\")\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "!pwd\n",
    "!ls -l\n",
    "if False:\n",
    "    path_dataset = get_or_download_dataset()\n",
    "    df_fake_1 = to_dataframe_spark(path_dataset + \"/gossipcop\"  + \"/fake\", 100000).toPandas()\n",
    "    print(\"df_fake_1 done\")\n",
    "    df_real_1 = to_dataframe_spark(path_dataset + \"/gossipcop\"  + \"/real\").toPandas()\n",
    "    print(\"df_real_1 done\")\n",
    "    df_fake_2 = to_dataframe_spark(path_dataset + \"/politifact\" + \"/fake\").toPandas()\n",
    "    print(\"df_fake_2 done\")\n",
    "    df_real_2 = to_dataframe_spark(path_dataset + \"/politifact\" + \"/real\").toPandas()\n",
    "    print(\"df_real_2 done\")\n",
    "    print_df([df_fake_1, df_real_1, df_fake_2, df_real_2])\n",
    "# lionel: weirdly, pandas' and spark's json APIs yield different number of columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# download or reference the dataset\n",
    "# ----------------------------------------\n",
    "def get_or_download_dataset():\n",
    "    # url = \"https://nusu-my.sharepoint.com/personal/e0809358_u_nus_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fe0809358%5Fu%5Fnus%5Fedu%2FDocuments%2FCS5344&ga=1\"\n",
    "    path = \"./dataset/group_by_user\"\n",
    "    return path\n",
    "# ----------------------------------------\n",
    "# convert to spark dataframe\n",
    "# ----------------------------------------\n",
    "def to_rdd_spark(path, head_len = None):\n",
    "    # lionel: out-of-memory\n",
    "    if head_len is None:\n",
    "        df_all = sqlcontext.read.json(path + \"/union/*.json\")\n",
    "        return df_all\n",
    "    directories = os.listdir(path)\n",
    "    json_files = []\n",
    "    df_all = spark.createDataFrame([], StructType([]))\n",
    "    print(\"reading first \" + str(head_len) + \" json files in \" + str(len(directories)) + \" directories in \" + path + \"...\")\n",
    "    for index,directory in enumerate(directories):\n",
    "        path_prefix = path + \"/\" + directory + \"/tweets\"\n",
    "        files = os.listdir(path_prefix)\n",
    "        # print(\"reading \" + str(len(files)) + \" json files from directory \" + directory + \" (\" + str(index) + \" of \" + str(len(directories)) + \")\")\n",
    "        for file in files:\n",
    "            json_files.append(path_prefix + \"/\" + file)\n",
    "        # lionel: fixed schema with columns we want must be known beforehand as columns are mismatched among json files\n",
    "        # df_current = sqlcontext.read.json(path_prefix + \"/*.json\")\n",
    "        # assert len(files) == df_current.count()\n",
    "        # df_all = df_all.unionByName(df_current, allowMissingColumns=True)\n",
    "        if head_len is not None and len(json_files) >= head_len:\n",
    "            break\n",
    "    if head_len is not None:\n",
    "        df_all = sqlcontext.read.json(json_files[:head_len])\n",
    "        assert len(json_files[:head_len]) == df_all.count()\n",
    "    else:\n",
    "        df_all = sqlcontext.read.json(json_files)\n",
    "    return df_all\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "!pwd\n",
    "!ls -l\n",
    "path_dataset = get_or_download_dataset()\n",
    "rdd_fake_1 = to_rdd_spark(path_dataset + \"/gossipcop\"  + \"/fake\")\n",
    "df_fake_1  = rdd_fake_1.toPandas()\n",
    "print(\"df_fake_1 done\")\n",
    "rdd_real_1 = to_rdd_spark(path_dataset + \"/gossipcop\"  + \"/real\")\n",
    "df_real_1  = rdd_real_1.toPandas()\n",
    "print(\"df_real_1 done\")\n",
    "rdd_fake_2 = to_rdd_spark(path_dataset + \"/politifact\" + \"/fake\")\n",
    "df_fake_2  = rdd_fake_2.toPandas()\n",
    "print(\"df_fake_2 done\")\n",
    "rdd_real_2 = to_rdd_spark(path_dataset + \"/politifact\" + \"/real\")\n",
    "df_real_2  = rdd_real_2.toPandas()\n",
    "print(\"df_real_2 done\")\n",
    "print_df([df_fake_1, df_real_1, df_fake_2, df_real_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e92906",
   "metadata": {},
   "source": [
    "### favourite_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2491bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def favourite_count(df_fake_1, df_real_1, df_fake_2, df_real_2):\n",
    "    X_fake_1_count = df_fake_1['favourites_count']\n",
    "    X_real_1_count = df_real_1['favourites_count']\n",
    "    X_fake_2_count = df_fake_2['favourites_count']\n",
    "    X_real_2_count = df_real_2['favourites_count']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plt.boxplot([\n",
    "        X_fake_1_count,\n",
    "        X_real_1_count,\n",
    "        X_fake_2_count,\n",
    "        X_real_2_count])\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels([\n",
    "        'gossipcop fake',\n",
    "        'gossipcop real',\n",
    "        'politifact fake',\n",
    "        'politifact real'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "favourite_count(df_fake_1, df_real_1, df_fake_2, df_real_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cf60b",
   "metadata": {},
   "source": [
    "### retweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retweet_count(df_fake_1, df_real_1, df_fake_2, df_real_2):\n",
    "    X_fake_1_count = df_fake_1['total_retweet_count']\n",
    "    X_real_1_count = df_real_1['total_retweet_count']\n",
    "    X_fake_2_count = df_fake_2['total_retweet_count']\n",
    "    X_real_2_count = df_real_2['total_retweet_count']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plt.boxplot([\n",
    "        X_fake_1_count,\n",
    "        X_real_1_count,\n",
    "        X_fake_2_count,\n",
    "        X_real_2_count])\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels([\n",
    "        'gossipcop fake',\n",
    "        'gossipcop real',\n",
    "        'politifact fake',\n",
    "        'politifact real'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "retweet_count(df_fake_1, df_real_1, df_fake_2, df_real_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0e156",
   "metadata": {},
   "source": [
    "### favorite_count to retweet_count (gossipcop fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa8a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def favorite_vs_retweet(df, title):\n",
    "    X_val = df['favourites_count']\n",
    "    Y_val = df['total_retweet_count']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.tick_params(labelsize=14)\n",
    "    plt.scatter(X_val, Y_val)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('favourites_count', fontsize=12)\n",
    "    plt.ylabel('total_retweet_count', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "favorite_vs_retweet(df_fake_1, \"gossipcop fake\")\n",
    "favorite_vs_retweet(df_real_1, \"gossipcop real\")\n",
    "favorite_vs_retweet(df_fake_2, \"politifact fake\")\n",
    "favorite_vs_retweet(df_real_2, \"politifact real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541ceee",
   "metadata": {},
   "source": [
    "### top words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_wordcounter(df, head_limit):\n",
    "    col_value = df['list_of_tweets']\n",
    "    row_count = len(col_value)\n",
    "    word_freq = Counter([])\n",
    "    tweet_tokenlist = []\n",
    "    for index, tweets in enumerate(col_value):\n",
    "        for curr_tweet in tweets:\n",
    "            curr_tweet_text = re.search(r\"^.*?,\\\"text\\\":\\\"(.*)\\\",\\\"truncated\\\".*$\", curr_tweet).group(1)\n",
    "            words = curr_tweet_text.split()\n",
    "            words = [word.lower() for word in words]\n",
    "            word_freq = word_freq + Counter(words)\n",
    "#             tweet_tokenlist.append(tuple(set(word_freq)))\n",
    "        if index % 1000 == 0:\n",
    "            print(str(round((index / row_count) * 100, 2)) + \"% of dataset\")\n",
    "        if index == head_limit:\n",
    "            break\n",
    "    return word_freq, tweet_tokenlist\n",
    "\n",
    "counter_fake_1, tokenlist_fake_1 = text_wordcounter(df_fake_1, 1000)\n",
    "print(\"df_fake_1 done\")\n",
    "counter_real_1, tokenlist_real_1 = text_wordcounter(df_real_1, 1000)\n",
    "print(\"df_real_1 done\")\n",
    "counter_fake_2, tokenlist_fake_2 = text_wordcounter(df_fake_2, 1000)\n",
    "print(\"df_fake_2 done\")\n",
    "counter_real_2, tokenlist_real_2 = text_wordcounter(df_real_2, 1000)\n",
    "print(\"df_real_2 done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656d171",
   "metadata": {},
   "source": [
    "### Association Rules with top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenlist_fake_1)\n",
    "# itemsets, rules = apriori(tokenlist_fake_1, min_support=0.9, min_confidence=0.9)\n",
    "# print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89efe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = {\n",
    "    'to', 'a', 'the', 'and', 'on', '-', 'is', 'with',\n",
    "    'i', 'of', 'you', '&amp;', 'for', 'out', 'air',\n",
    "    'win', 'in', 'her', 'tv', 'at', 'after', 'best',\n",
    "    'your', 'first', 'new', 'about', 'found', 'as',\n",
    "    'how', 'via', 'my', 'by', 'if', 'one', 'four',\n",
    "    'that', 'have', 'was', 'this', 'are', 'be', 'it',\n",
    "    'has', 'from', 'all', 'but', 'just', 'not', 'u.s.',\n",
    "    'only', 'two', 'more', 'will', 'an', 'me', 'had',\n",
    "    'like', 'we', 'so', 'been', 'our', 'or', 'three',\n",
    "    \"i'm\", 'years', 'see', 'top', '#eonline', '2018',\n",
    "    'says', 'us', 'https', 'news', 't', 'co', 's',\n",
    "    'https://t.co/g79yrlmstd', 'https://t.co/pas8l48opb'\n",
    "    }\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords |= additional_stopwords\n",
    "\n",
    "for stopword in stopwords:\n",
    "    del counter_fake_1[stopword]\n",
    "    del counter_real_1[stopword]\n",
    "    del counter_fake_2[stopword]\n",
    "    del counter_real_2[stopword]\n",
    "    \n",
    "print(\"---- number of unqiue words ----\")\n",
    "print(len(counter_fake_1))\n",
    "print(len(counter_real_1))\n",
    "print(len(counter_fake_2))\n",
    "print(len(counter_real_2))\n",
    "\n",
    "k_count = 10\n",
    "topk_fake_1 = counter_fake_1.most_common(k_count)\n",
    "topk_real_1 = counter_real_1.most_common(k_count)\n",
    "topk_fake_2 = counter_fake_2.most_common(k_count)\n",
    "topk_real_2 = counter_real_2.most_common(k_count)\n",
    "\n",
    "print(\"---- \" + \"gossipcop fake\" + \" ----\")\n",
    "print(*topk_fake_1, sep = \"\\n\")\n",
    "print(\"---- \" + \"gossipcop real\" + \" ----\")\n",
    "print(*topk_real_1, sep = \"\\n\")\n",
    "print(\"---- \" + \"politifact fake\" + \" ----\")\n",
    "print(*topk_fake_2, sep = \"\\n\")\n",
    "print(\"---- \" + \"politifact real\" + \" ----\")\n",
    "print(*topk_real_2, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821894f",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_fake_1 = \" \".join(counter_fake_1.elements())\n",
    "string_real_1 = \" \".join(counter_real_1.elements())\n",
    "string_fake_2 = \" \".join(counter_fake_2.elements())\n",
    "string_real_2 = \" \".join(counter_real_2.elements())\n",
    "\n",
    "wc_string_list = [string_fake_1, string_real_1, string_fake_2, string_real_2]\n",
    "for wc_string in wc_string_list:\n",
    "    wordcloud_fake_1 = WordCloud(\n",
    "        width = 800, height = 800, background_color ='white',\n",
    "        collocations = False, stopwords = stopwords,\n",
    "        min_font_size = 10).generate(wc_string)\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud_fake_1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4bd6e7",
   "metadata": {},
   "source": [
    "### top id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e21528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def top_id_str(df, title):\n",
    "#     df_top_id = df[['id_str', 'favorite_count', 'retweet_count']]\n",
    "#     print(\"---- \" + title + \" ----\")\n",
    "#     df_top_id = df_top_id.groupby('id_str')['favorite_count', 'retweet_count'].agg('count')\n",
    "#     print(df_top_id)\n",
    "\n",
    "# top_id_str(df_fake_1, \"gossipcop fake\")\n",
    "# top_id_str(df_real_1, \"gossipcop real\")\n",
    "# top_id_str(df_fake_2, \"politifact fake\")\n",
    "# top_id_str(df_real_2, \"politifact real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fca69a",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbe43f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2d2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
