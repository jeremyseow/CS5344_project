{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637d17b1",
   "metadata": {},
   "source": [
    "## FakeNewsNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af22c9b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports\n",
    "import os.path\n",
    "import urllib.request\n",
    "import shutil\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "# external library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# project imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ea7d7",
   "metadata": {},
   "source": [
    "### PySpark contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2467a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FakeNewsNet').getOrCreate()\n",
    "sparkcontext = spark.sparkContext\n",
    "sqlcontext   = SQLContext(sparkcontext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56b3e7",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# download or reference the dataset\n",
    "# ----------------------------------------\n",
    "def get_or_download_dataset():\n",
    "    path = \"./dataset/tweets\"\n",
    "    if not os.path.exists(\"./dataset\"):\n",
    "        print(\"downloading tweets...\")\n",
    "        os.mkdir(\"./dataset\")\n",
    "        # lionel: no public access to download the dataset directly. manually download and place in the stated directory\n",
    "        url = \"https://nusu-my.sharepoint.com/:u:/g/personal/e0809358_u_nus_edu/ETPkp1-0GbBHgB__wyeCS_QBE7_SFluzSCtocU0mUr3Jng\"\n",
    "        with urllib.request.urlopen(url) as response, open(path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "            with zipfile.ZipFile(path + \"/results.zip\", \"r\") as zip_ref:\n",
    "                zip_ref.extractall(path)\n",
    "    return path\n",
    "# ----------------------------------------\n",
    "# convert to pandas dataframe\n",
    "# ----------------------------------------\n",
    "def to_dataframe_pandas(path, head_len = None):\n",
    "    directories = os.listdir(path)\n",
    "    json_all = []\n",
    "    df_all = pd.DataFrame\n",
    "    print(\"reading first \" + str(head_len) + \" json files in \" + str(len(directories)) + \" directories in \" + path + \"...\")\n",
    "    for index,directory in enumerate(directories):\n",
    "        path_prefix = path + \"/\" + directory + \"/tweets\"\n",
    "        files = os.listdir(path_prefix)\n",
    "        # print(\"reading \" + str(len(files)) + \" json files from directory \" + directory + \" (\" + str(index) + \" of \" + str(len(directories)) + \")\")\n",
    "        json_lines = []\n",
    "        for file in files:\n",
    "            path_full = path_prefix + \"/\" + file\n",
    "            with open(path_full, 'r') as json_file:\n",
    "                json_lines.append(json.loads(json_file.read()))\n",
    "        df_current = pd.json_normalize(json_lines)\n",
    "        assert len(files) == len(json_lines)\n",
    "        assert len(files) == df_current.shape[0]\n",
    "        # lionel: fixed schema with columns we want must be known beforehand as columns are mismatched among json files\n",
    "        # df_all = pd.concat([df_all, df_current], axis=0, join='outer', sort=False)\n",
    "        json_all.extend(json_lines)\n",
    "        if head_len is not None and len(json_all) >= head_len:\n",
    "            break\n",
    "    if head_len is not None:\n",
    "        df_all = pd.json_normalize(json_all[:head_len])\n",
    "    else:\n",
    "        df_all = pd.json_normalize(json_all)\n",
    "    return df_all\n",
    "# ----------------------------------------\n",
    "# convert to spark dataframe\n",
    "# ----------------------------------------\n",
    "def to_dataframe_spark(path, head_len = None):\n",
    "    # lionel: out-of-memory\n",
    "    # df_all = sqlcontext.read.json(path + \"/*/tweets/*.json\")\n",
    "    # return df_all\n",
    "    directories = os.listdir(path)\n",
    "    json_files = []\n",
    "    df_all = spark.createDataFrame([], StructType([]))\n",
    "    print(\"reading first \" + str(head_len) + \" json files in \" + str(len(directories)) + \" directories in \" + path + \"...\")\n",
    "    for index,directory in enumerate(directories):\n",
    "        path_prefix = path + \"/\" + directory + \"/tweets\"\n",
    "        files = os.listdir(path_prefix)\n",
    "        # print(\"reading \" + str(len(files)) + \" json files from directory \" + directory + \" (\" + str(index) + \" of \" + str(len(directories)) + \")\")\n",
    "        for file in files:\n",
    "            json_files.append(path_prefix + \"/\" + file)\n",
    "        # lionel: fixed schema with columns we want must be known beforehand as columns are mismatched among json files\n",
    "        # df_current = sqlcontext.read.json(path_prefix + \"/*.json\")\n",
    "        # assert len(files) == df_current.count()\n",
    "        # df_all = df_all.unionByName(df_current, allowMissingColumns=True)\n",
    "        if head_len is not None and len(json_files) >= head_len:\n",
    "            break\n",
    "    if head_len is not None:\n",
    "        df_all = sqlcontext.read.json(json_files[:head_len])\n",
    "        assert len(json_files[:head_len]) == df_all.count()\n",
    "    else:\n",
    "        df_all = sqlcontext.read.json(json_files)\n",
    "    return df_all\n",
    "# ----------------------------------------\n",
    "# print helper function\n",
    "# ----------------------------------------\n",
    "def print_df(dfs):\n",
    "    for df in dfs:\n",
    "        print(\"==========\")\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            print(df.shape)\n",
    "            print(df.info())\n",
    "            print(df.columns)\n",
    "            print(df.describe())\n",
    "        if isinstance(df, DataFrame):\n",
    "            print(\"(\" + str(df.count()) + \",\" + str(len(df.columns)) + \")\")\n",
    "            print(df.columns)\n",
    "            print(df.summary().show())\n",
    "    print(\"==========\")\n",
    "# ----------------------------------------\n",
    "# call sites\n",
    "# ----------------------------------------\n",
    "!pwd\n",
    "!ls -l\n",
    "path_dataset = get_or_download_dataset()\n",
    "df_fake_1 = to_dataframe_spark(path_dataset + \"/gossipcop\"  + \"/fake\", 10000)\n",
    "df_real_1 = to_dataframe_spark(path_dataset + \"/gossipcop\"  + \"/real\", 10000)\n",
    "df_fake_2 = to_dataframe_spark(path_dataset + \"/politifact\" + \"/fake\", 10000)\n",
    "df_real_2 = to_dataframe_spark(path_dataset + \"/politifact\" + \"/real\", 10000)\n",
    "df_fake_1 = df_fake_1.toPandas()\n",
    "df_real_1 = df_real_1.toPandas()\n",
    "df_fake_2 = df_fake_2.toPandas()\n",
    "df_real_2 = df_real_2.toPandas()\n",
    "print_df([df_fake_1, df_real_1, df_fake_2, df_real_2])\n",
    "# lionel: weirdly, pandas' and spark's json APIs yield different number of columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e92906",
   "metadata": {},
   "source": [
    "### favourite_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2491bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def favourite_count(df_fake_1, df_real_1, df_fake_2, df_real_2):\n",
    "    X_fake_1_count = df_fake_1['favorite_count']\n",
    "    X_real_1_count = df_real_1['favorite_count']\n",
    "    X_fake_2_count = df_fake_2['favorite_count']\n",
    "    X_real_2_count = df_real_2['favorite_count']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plt.boxplot([\n",
    "        X_fake_1_count,\n",
    "        X_real_1_count,\n",
    "        X_fake_2_count,\n",
    "        X_real_2_count])\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels([\n",
    "        'gossipcop fake',\n",
    "        'gossipcop real',\n",
    "        'politifact fake',\n",
    "        'politifact real'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "favourite_count(df_fake_1, df_real_1, df_fake_2, df_real_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cf60b",
   "metadata": {},
   "source": [
    "### retweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retweet_count(df_fake_1, df_real_1, df_fake_2, df_real_2):\n",
    "    X_fake_1_count = df_fake_1['retweet_count']\n",
    "    X_real_1_count = df_real_1['retweet_count']\n",
    "    X_fake_2_count = df_fake_2['retweet_count']\n",
    "    X_real_2_count = df_real_2['retweet_count']\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plt.boxplot([\n",
    "        X_fake_1_count,\n",
    "        X_real_1_count,\n",
    "        X_fake_2_count,\n",
    "        X_real_2_count])\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels([\n",
    "        'gossipcop fake',\n",
    "        'gossipcop real',\n",
    "        'politifact fake',\n",
    "        'politifact real'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "retweet_count(df_fake_1, df_real_1, df_fake_2, df_real_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0e156",
   "metadata": {},
   "source": [
    "### favorite_count to retweet_count (gossipcop fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def favorite_vs_retweet(df, title):\n",
    "    X_val = df['favorite_count']\n",
    "    Y_val = df['retweet_count']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.tick_params(labelsize=14)\n",
    "    plt.scatter(X_val, Y_val)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('favorite_count', fontsize=12)\n",
    "    plt.ylabel('retweet_count', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "favorite_vs_retweet(df_fake_1, \"gossipcop fake\")\n",
    "favorite_vs_retweet(df_real_1, \"gossipcop real\")\n",
    "favorite_vs_retweet(df_fake_2, \"politifact fake\")\n",
    "favorite_vs_retweet(df_real_2, \"politifact real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541ceee",
   "metadata": {},
   "source": [
    "### top words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_interesting_text(df, title):\n",
    "    col_text = df['text']\n",
    "    word_freq = {}\n",
    "    for curr_text in col_text:\n",
    "        words = curr_text.split()\n",
    "        for word in words:\n",
    "            if word not in word_freq:\n",
    "                word_freq[word] = 0\n",
    "            word_freq[word] += 1\n",
    "    top_words = sorted(word_freq.items(), key = lambda item : item[1], reverse=True)\n",
    "    disable_set = {'to', 'a', 'the', 'and', 'on', '-', 'is', 'with',\n",
    "                   'i', 'of', 'you', '&amp;', 'for', 'out', 'air',\n",
    "                   'win', 'in', 'her', 'tv', 'at', 'after', 'best',\n",
    "                   'your', 'first', 'new', 'about', 'found', 'as',\n",
    "                   'how', 'via', 'my', 'by', 'if', 'one', 'four',\n",
    "                   'that', 'have', 'was', 'this', 'are', 'be', 'it',\n",
    "                   'has', 'from', 'all', 'but', 'just', 'not',\n",
    "                   'only', 'two', 'more', 'will', 'an', 'me', 'had',\n",
    "                   'like', 'we', 'so', 'been', 'our', 'or', 'three',\n",
    "                   \"i'm\", 'years'\n",
    "                  }\n",
    "    k_count = 10\n",
    "    print(\"---- \" + title + \" ----\")\n",
    "    for elem in top_words:\n",
    "        if elem[0].lower() in disable_set:\n",
    "            continue\n",
    "        print(elem)\n",
    "        k_count -= 1\n",
    "        if k_count == 0:\n",
    "            break\n",
    "    \n",
    "top_interesting_text(df_fake_1, \"gossipcop fake\")\n",
    "top_interesting_text(df_real_1, \"gossipcop real\")\n",
    "top_interesting_text(df_fake_2, \"politifact fake\")\n",
    "top_interesting_text(df_real_2, \"politifact real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4bd6e7",
   "metadata": {},
   "source": [
    "### top id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e21528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_id_str(df, title):\n",
    "    df_top_id = df[['id_str', 'favorite_count', 'retweet_count']]\n",
    "    print(\"---- \" + title + \" ----\")\n",
    "    df_top_id = df_top_id.groupby('id_str')['favorite_count', 'retweet_count'].agg('count')\n",
    "    print(df_top_id)\n",
    "\n",
    "top_id_str(df_fake_1, \"gossipcop fake\")\n",
    "top_id_str(df_real_1, \"gossipcop real\")\n",
    "top_id_str(df_fake_2, \"politifact fake\")\n",
    "top_id_str(df_real_2, \"politifact real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fca69a",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbe43f",
   "metadata": {},
   "source": [
    "### Association Rules with top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2d2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
